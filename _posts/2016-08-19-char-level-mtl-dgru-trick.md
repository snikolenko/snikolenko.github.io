---
layout: post
title: State-forgetting trick for char-level NMT
excerpt: ""
categories: papers
paper_years: 2016
tags: [rnn, char-level, nmt, arxiv-cl-news]
author: alexeyev
comments: true
---

[A recent arxiv paper](http://arxiv.org/abs/1608.04738) on character-level neural machine translation via an encoding-decoding model with a little sampling trick: RNNs' state is reset to a starting one at chosen 'delimiters' positions \[...review is to be fixed after reading the paper\].

On review for ICLR'2017.
