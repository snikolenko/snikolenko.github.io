# LSTM for Machine Reading
(The June paper of NLP group from Edinburgh)[http://arxiv.org/abs/1601.06733] reports another application of trendy memory networks in combination with the LSTM unit. The idea of LSTMN is quite simple: instead of standard vector used as a memory cell in LSTM we use memory network block, e.g. an array of cells with a softmax layer that focuses on the elements of this array. The latter structure is then used in encoder-decoder architectures of two types: one with “shallow fusion”, where encoder LSTM is connected with decoder only at the very end, and another with “deep fusion”, where the encoder’s output is used for computing attention inside of decoder. Good results are shown for language modelling, sentiment analysis and identifying semantic relationships between two sentences, especially with deep stacked networks (but different kinds of fusion doesn’t seem to make a difference). What surprises me, however, is that the comparison is only performed against other kinds of LSTM (like grid and gated) and CNN, but not against other memory networks. Memory network is quite a complex structure, does it really makes sense also use LSTM on top of that instead of simple (RNN [Tran et al. 2016])[http://arxiv.org/abs/1601.01272]? Quite possibly it does, but it would be interesting to try. And what are the other ways of writing the memory if we’re not using an external one if it’s not a recurrent network -- where else do we get the memories?
In general I would assume that this would be a to-go structure in NLP now, cuz memory networks for sequences, yay.
